{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with scrapin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping python.org with Requests and Beauritifsoup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe we will install Requests and Beautiful Soup and scrape some content from www.python.org.  We'll install both of the libraries and get some basic familiarity with them.  We'll come back to them both in subsequent chapters and dive deeper into each."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go and learn to scrape a couple events. For this recipe we will start by using interactive python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Import requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 We now use requests to make a GET HTTP request for the url by making a GET requests\n",
    "url = 'https://www.python.org/events/python-events'\n",
    "req = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html>\\n<!--[if lt IE 7]>   <html class=\"no-js ie6 lt-ie7 lt-ie8 lt-ie9\">   <![endif]-->\\n<!--[if IE 7]>      <html class=\"no-js ie7 lt-ie8 lt-ie9\">          <![endif]-->\\n<!--[if IE 8]>      <h'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 That downloaded the page content but it is stored in our requests object req.\n",
    "# We can retrieve the content using the\n",
    "# .text property.  This prints the first 200 characters.\n",
    "req.text[:200]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the raw HTML of the page.  We can now use beautiful soup to parse the HTML and retrieve the event data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'PyConFr 2023', 'location': 'Bordeaux, France', 'time': '16 Feb. – 19 Feb.  2023'}\n",
      "{'name': 'PyCon Namibia 2023', 'location': 'Windhoek, Namibia', 'time': '21 Feb. – 23 Feb.  2023'}\n",
      "{'name': 'PyCon PH 2023', 'location': 'Manila, Philippines', 'time': '25 Feb. – 26 Feb.  2023'}\n",
      "{'name': 'GeoPython 2023', 'location': 'Basel, Switzerland', 'time': '06 March – 08 March  2023'}\n",
      "{'name': 'PyCon DE & PyData Berlin 2023', 'location': 'Berlin, Germany', 'time': '17 April – 19 April  2023'}\n",
      "{'name': 'PyCon US 2023', 'location': 'Salt Lake City, Utah, USA', 'time': '19 April – 27 April  2023'}\n"
     ]
    }
   ],
   "source": [
    "# 1 First let's import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "# 2 Now we create a BeautifulSoup object and pass it the HTML.\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "# 3 Now we tell Beautiful Soup to find the main <ul> tag for the recent events, and then to get all the <li> tags below it.\n",
    "events = soup.find('ul', {'class': 'list-recent-events'}).findAll('li')\n",
    "# 4 And finally we can loop through each of the <li> elements, extracting the event details, and print each to the console:\n",
    "for event in events:\n",
    "    event_details = dict()\n",
    "    event_details['name'] = event.find('h3').find('a').text\n",
    "    event_details['location'] = event.find('span', {'class', 'event-location'}).text\n",
    "    event_details['time'] = event.find('time').text\n",
    "    print(event_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will dive into details of both Requests and Beautiful Soup in the next chapter, but for now let's just summarize a few key points about how this works.  The following important points about Requests:\n",
    "\n",
    "* Requests is used to execute HTTP requests.  We used it to make a GET verb request of the URL for the events page.\n",
    "* The Requests object holds the results of the request.  This is not only the page content, but also many other items about the result such as HTTP status codes and headers.\n",
    "* Requests is used only to get the page, it does not do an parsing.\n",
    "\n",
    "We use Beautiful Soup to do the parsing of the HTML and also the finding of content within the HTML.\n",
    "\n",
    "We used the power of Beautiful Soup to:\n",
    "\n",
    "* Find the `<ul>` element representing the section, which is found by looking for a `<ul>` with the a class attribute that has a value of list-recent-events.\n",
    "* From that object, we find all the `<li>` elements. \n",
    "\n",
    "Each of these `<li>` tags represent a different event.  We iterate over each of those making a dictionary from the event data found in child HTML tags:\n",
    "\n",
    "* The name is extracted from the `<a>` tag that is a child of the `<h3>` tag\n",
    "* The location is the text content of the `<span>` with a class of `event-location`\n",
    "And the time is extracted from the datetime attribute of the `<time>` tag."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping Python.org in urllib3 and Beautiful Soup**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe we swap out the use of requests for another library `urllib3`. This is **another common library for retrieving data from URLs and for other functions involving URLs such as parsing of the parts of the actual URL and handling various encodings**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting ready**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in /home/ibrahim/miniconda3/envs/scraping/lib/python3.10/site-packages (1.26.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'PyConFr 2023', 'location': 'Bordeaux, France', 'time': '16 Feb. – 19 Feb.  2023'}\n",
      "{'name': 'PyCon Namibia 2023', 'location': 'Windhoek, Namibia', 'time': '21 Feb. – 23 Feb.  2023'}\n",
      "{'name': 'PyCon PH 2023', 'location': 'Manila, Philippines', 'time': '25 Feb. – 26 Feb.  2023'}\n",
      "{'name': 'GeoPython 2023', 'location': 'Basel, Switzerland', 'time': '06 March – 08 March  2023'}\n",
      "{'name': 'PyCon DE & PyData Berlin 2023', 'location': 'Berlin, Germany', 'time': '17 April – 19 April  2023'}\n",
      "{'name': 'PyCon US 2023', 'location': 'Salt Lake City, Utah, USA', 'time': '19 April – 27 April  2023'}\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_upcoming_events(url):\n",
    "    req = urllib3.PoolManager()\n",
    "    res = req.request('GET', url)\n",
    "    \n",
    "    soup = BeautifulSoup(res.data, 'html.parser')\n",
    "\n",
    "    events = soup.find('ul', {'class': 'list-recent-events'}).findAll('li')\n",
    "\n",
    "    for event in events:\n",
    "        event_details = dict()\n",
    "        event_details['name'] = event.find('h3').find(\"a\").text\n",
    "        event_details['location'] = event.find('span', {'class', 'event-location'}).text\n",
    "        event_details['time'] = event.find('time').text\n",
    "        print(event_details)\n",
    "\n",
    "get_upcoming_events('https://www.python.org/events/python-events/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference in this recipe is how we fetch the resource:\n",
    "\n",
    "```python\n",
    "req = urllib3.PoolManager()\n",
    "res = req.request('GET', url)\n",
    "```\n",
    "\n",
    "Unlike `Requests`, `urllib3` **doesn't apply header encoding automatically**. **The reason why the code snippet works in the preceding example is because BS4 handles encoding beautifully**.  But you should keep in mind that **encoding is an important part of scraping**. **If you decide to use your own framework or use other libraries, make sure encoding is well handled**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **There's more**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Requests` and `urllib3` are very similar in terms of capabilities. **it is generally recommended to use Requests when it comes to making HTTP requests**. The following code example illustrates a few advanced features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "# builds on top of urllib3's connection pooling\n",
    "# session reuses the same TCP connection if \n",
    "# requests are made to the same host\n",
    "# see https://en.wikipedia.org/wiki/HTTP_persistent_connection for details\n",
    "session = requests.Session()\n",
    "\n",
    "# You may pass in custom cookie\n",
    "r = session.get('http://httpbin.org/get', cookies={'my-cookie': 'browser'})\n",
    "print(r.text)\n",
    "# '{\"cookies\": {\"my-cookie\": \"test cookie\"}}'\n",
    "\n",
    "# Streaming is another nifty feature\n",
    "# From http://docs.python-requests.org/en/master/user/advanced/#streaming-requests\n",
    "# copyright belongs to reques.org\n",
    "r = requests.get('http://httpbin.org/stream/20', stream=True)\n",
    "\n",
    "for line in r.iter_lines():\n",
    "    # filter out keep-alive new lines\n",
    "    if line:\n",
    "        decoded_line = line.decode('utf-8')\n",
    "        print(json.loads(decoded_line))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2e900357f7d7f0420973cc0a3db668cd1358155e01ada4ae7b047d426ff3a9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
