{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping python.org with Requests and Beautifulsoup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe we will install Requests and Beautiful Soup and scrape some content from www.python.org.  We'll install both of the libraries and get some basic familiarity with them.  We'll come back to them both in subsequent chapters and dive deeper into each."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go and learn to scrape a couple events. For this recipe we will start by using interactive python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Import requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 We now use requests to make a GET HTTP request for the url by making a GET requests\n",
    "url = 'https://www.python.org/events/python-events'\n",
    "req = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html>\\n<!--[if lt IE 7]>   <html class=\"no-js ie6 lt-ie7 lt-ie8 lt-ie9\">   <![endif]-->\\n<!--[if IE 7]>      <html class=\"no-js ie7 lt-ie8 lt-ie9\">          <![endif]-->\\n<!--[if IE 8]>      <h'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 That downloaded the page content but it is stored in our requests object req.\n",
    "# We can retrieve the content using the\n",
    "# .text property.  This prints the first 200 characters.\n",
    "req.text[:200]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the raw HTML of the page.  We can now use beautiful soup to parse the HTML and retrieve the event data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'PyConFr 2023', 'location': 'Bordeaux, France', 'time': '16 Feb. – 19 Feb.  2023'}\n",
      "{'name': 'PyCon Namibia 2023', 'location': 'Windhoek, Namibia', 'time': '21 Feb. – 23 Feb.  2023'}\n",
      "{'name': 'PyCon PH 2023', 'location': 'Manila, Philippines', 'time': '25 Feb. – 26 Feb.  2023'}\n",
      "{'name': 'GeoPython 2023', 'location': 'Basel, Switzerland', 'time': '06 March – 08 March  2023'}\n",
      "{'name': 'PyCon DE & PyData Berlin 2023', 'location': 'Berlin, Germany', 'time': '17 April – 19 April  2023'}\n",
      "{'name': 'PyCon US 2023', 'location': 'Salt Lake City, Utah, USA', 'time': '19 April – 27 April  2023'}\n"
     ]
    }
   ],
   "source": [
    "# 1 First let's import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "# 2 Now we create a BeautifulSoup object and pass it the HTML.\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "# 3 Now we tell Beautiful Soup to find the main <ul> tag for the recent events, and then to get all the <li> tags below it.\n",
    "events = soup.find('ul', {'class': 'list-recent-events'}).findAll('li')\n",
    "# 4 And finally we can loop through each of the <li> elements, extracting the event details, and print each to the console:\n",
    "for event in events:\n",
    "    event_details = dict()\n",
    "    event_details['name'] = event.find('h3').find('a').text\n",
    "    event_details['location'] = event.find('span', {'class', 'event-location'}).text\n",
    "    event_details['time'] = event.find('time').text\n",
    "    print(event_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will dive into details of both Requests and Beautiful Soup in the next chapter, but for now let's just summarize a few key points about how this works.  The following important points about Requests:\n",
    "\n",
    "* Requests is used to execute HTTP requests.  We used it to make a GET verb request of the URL for the events page.\n",
    "* The Requests object holds the results of the request.  This is not only the page content, but also many other items about the result such as HTTP status codes and headers.\n",
    "* Requests is used only to get the page, it does not do an parsing.\n",
    "\n",
    "We use Beautiful Soup to do the parsing of the HTML and also the finding of content within the HTML.\n",
    "\n",
    "We used the power of Beautiful Soup to:\n",
    "\n",
    "* Find the `<ul>` element representing the section, which is found by looking for a `<ul>` with the a class attribute that has a value of list-recent-events.\n",
    "* From that object, we find all the `<li>` elements. \n",
    "\n",
    "Each of these `<li>` tags represent a different event.  We iterate over each of those making a dictionary from the event data found in child HTML tags:\n",
    "\n",
    "* The name is extracted from the `<a>` tag that is a child of the `<h3>` tag\n",
    "* The location is the text content of the `<span>` with a class of `event-location`\n",
    "And the time is extracted from the datetime attribute of the `<time>` tag."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping Python.org in urllib3 and Beautiful Soup**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe we swap out the use of requests for another library `urllib3`. This is **another common library for retrieving data from URLs and for other functions involving URLs such as parsing of the parts of the actual URL and handling various encodings**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting ready**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3 in c:\\users\\hp\\miniconda3\\envs\\scraping\\lib\\site-packages (1.26.13)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'PyConFr 2023', 'location': 'Bordeaux, France', 'time': '16 Feb. – 19 Feb.  2023'}\n",
      "{'name': 'PyCon Namibia 2023', 'location': 'Windhoek, Namibia', 'time': '21 Feb. – 23 Feb.  2023'}\n",
      "{'name': 'PyCon PH 2023', 'location': 'Manila, Philippines', 'time': '25 Feb. – 26 Feb.  2023'}\n",
      "{'name': 'GeoPython 2023', 'location': 'Basel, Switzerland', 'time': '06 March – 08 March  2023'}\n",
      "{'name': 'PyCon DE & PyData Berlin 2023', 'location': 'Berlin, Germany', 'time': '17 April – 19 April  2023'}\n",
      "{'name': 'PyCon US 2023', 'location': 'Salt Lake City, Utah, USA', 'time': '19 April – 27 April  2023'}\n"
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_upcoming_events(url):\n",
    "    req = urllib3.PoolManager()\n",
    "    res = req.request('GET', url)\n",
    "    \n",
    "    soup = BeautifulSoup(res.data, 'html.parser')\n",
    "\n",
    "    events = soup.find('ul', {'class': 'list-recent-events'}).findAll('li')\n",
    "\n",
    "    for event in events:\n",
    "        event_details = dict()\n",
    "        event_details['name'] = event.find('h3').find(\"a\").text\n",
    "        event_details['location'] = event.find('span', {'class', 'event-location'}).text\n",
    "        event_details['time'] = event.find('time').text\n",
    "        print(event_details)\n",
    "\n",
    "get_upcoming_events('https://www.python.org/events/python-events/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference in this recipe is how we fetch the resource:\n",
    "\n",
    "```python\n",
    "req = urllib3.PoolManager()\n",
    "res = req.request('GET', url)\n",
    "```\n",
    "\n",
    "Unlike `Requests`, `urllib3` **doesn't apply header encoding automatically**. **The reason why the code snippet works in the preceding example is because BS4 handles encoding beautifully**.  But you should keep in mind that **encoding is an important part of scraping**. **If you decide to use your own framework or use other libraries, make sure encoding is well handled**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **There's more**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Requests` and `urllib3` are very similar in terms of capabilities. **it is generally recommended to use Requests when it comes to making HTTP requests**. The following code example illustrates a few advanced features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
      "    \"Cookie\": \"my-cookie=browser\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.28.1\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-63b94f65-69d0e206189ba7c9563e35b5\"\n",
      "  }, \n",
      "  \"origin\": \"102.23.26.168\", \n",
      "  \"url\": \"http://httpbin.org/get\"\n",
      "}\n",
      "\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 0}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 1}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 2}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 3}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 4}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 5}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 6}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 7}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 8}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 9}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 10}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 11}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 12}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 13}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 14}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 15}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 16}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 17}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 18}\n",
      "{'url': 'http://httpbin.org/stream/20', 'args': {}, 'headers': {'Host': 'httpbin.org', 'X-Amzn-Trace-Id': 'Root=1-63b94f66-3c4bf7a058baa26b74c80d7e', 'User-Agent': 'python-requests/2.28.1', 'Accept-Encoding': 'gzip, deflate, br', 'Accept': '*/*'}, 'origin': '102.23.26.168', 'id': 19}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "# builds on top of urllib3's connection pooling\n",
    "# session reuses the same TCP connection if \n",
    "# requests are made to the same host\n",
    "# see https://en.wikipedia.org/wiki/HTTP_persistent_connection for details\n",
    "session = requests.Session()\n",
    "\n",
    "# You may pass in custom cookie\n",
    "r = session.get('http://httpbin.org/get', cookies={'my-cookie': 'browser'})\n",
    "print(r.text)\n",
    "# '{\"cookies\": {\"my-cookie\": \"test cookie\"}}'\n",
    "\n",
    "# Streaming is another nifty feature\n",
    "# From http://docs.python-requests.org/en/master/user/advanced/#streaming-requests\n",
    "# copyright belongs to reques.org\n",
    "r = requests.get('http://httpbin.org/stream/20', stream=True)\n",
    "\n",
    "for line in r.iter_lines():\n",
    "    # filter out keep-alive new lines\n",
    "    if line:\n",
    "        decoded_line = line.decode('utf-8')\n",
    "        print(json.loads(decoded_line))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping Python.org with Scrapy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy is a very popular open source Python scraping framework for extracting data. **It was originally designed for only scraping, but it is has also evolved into a powerful web crawling solution**.\n",
    "\n",
    "In our previous recipes, we used Requests and urllib2 to fetch data and Beautiful Soup to extract data. **Scrapy offers all of these functionalities with many other built-in modules and extensions. It is also our tool of choice when it comes to scraping with Python**. \n",
    "\n",
    "Scrapy offers a number of powerful features that are worth mentioning:\n",
    "\n",
    "* Built-in extensions to make HTTP requests and handle compression, authentication, caching, manipulate user-agents, and HTTP headers\n",
    "* Built-in support for selecting and extracting data with selector languages such as CSS and XPath, as well as support for utilizing regular expressions for selection of content and links \n",
    "* Encoding support to deal with languages and non-standard encoding declarations\n",
    "* Flexible APIs to reuse and write custom middleware and pipelines, which provide a clean and easy way to implement tasks such as automatically downloading assets (for example, images or media) and storing data in storage such as file systems, S3, databases, and others"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting started**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several means of creating a scraper with Scrapy.  **One is a programmatic pattern where we create the crawler and spider in our code**.  It is also possible to **configure a Scrapy project from templates or generators and then run the scraper from the command line using the scrapy command**.  This book will follow the programmatic pattern as it contains the code in a single file more effectively.  This will help when we are putting together specific, targeted, recipes with Scrapy. \n",
    "\n",
    "**This isn't necessarily a better way of running a Scrapy scraper than using the command line execution, just one that is a design decision for this book**.  Ultimately this book is not about Scrapy (there are other books on just Scrapy), but more of an exposition on various things you may need to do when scraping, and in the ultimate creation of a functional scraper as a service in the cloud."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'PyConFr 2023', 'location': 'Bordeaux, France', 'time': '16 Feb. – 19 Feb. '}\n",
      "{'name': 'PyCon Namibia 2023', 'location': 'Windhoek, Namibia', 'time': '21 Feb. – 23 Feb. '}\n",
      "{'name': 'PyCon PH 2023', 'location': 'Manila, Philippines', 'time': '25 Feb. – 26 Feb. '}\n",
      "{'name': 'GeoPython 2023', 'location': 'Basel, Switzerland', 'time': '06 March – 08 March '}\n",
      "{'name': 'PyCon DE & PyData Berlin 2023', 'location': 'Berlin, Germany', 'time': '17 April – 19 April '}\n",
      "{'name': 'PyCon US 2023', 'location': 'Salt Lake City, Utah, USA', 'time': '19 April – 27 April '}\n",
      "{'name': 'XtremePython 2022', 'location': 'Online', 'time': '27 Dec.'}\n",
      "{'name': 'PyCon Bolivia 2022', 'location': 'Cochabamba, Bolivia', 'time': '09 Dec. – 10 Dec. '}\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class PythonEventsSpider(scrapy.Spider):\n",
    "    name = 'pythoneventsspider'\n",
    "\n",
    "    start_urls = ['https://www.python.org/events/python-events/',]\n",
    "    found_events = []\n",
    "\n",
    "    def parse(self, response):\n",
    "        for event in response.xpath('//ul[contains(@class, \"list-recent-events\")]/li'):\n",
    "            event_details = dict()\n",
    "            event_details['name'] = event.xpath('h3[@class=\"event-title\"]/a/text()').extract_first()\n",
    "            event_details['location'] = event.xpath('p/span[@class=\"event-location\"]/text()').extract_first()\n",
    "            event_details['time'] = event.xpath('p/time/text()').extract_first()\n",
    "            self.found_events.append(event_details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = CrawlerProcess({ 'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(PythonEventsSpider)\n",
    "    spider = next(iter(process.crawlers)).spider\n",
    "    process.start()\n",
    "\n",
    "    for event in spider.found_events: print(event)\n",
    "    process.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get into some details about Scrapy in later chapters, but let's just go through this code quick to get a feel how it is accomplishing this scrape.  Everything in Scrapy revolves around creating a spider.  Spiders crawl through pages on the Internet based upon rules that we provide.  This spider only processes one single page, so it's not really much of a spider.  But it shows the pattern we will use through later Scrapy examples.\n",
    "\n",
    "The spider is created with a class definition that derives from one of the Scrapy spider classes.  Ours derives from the scrapy.Spider class.\n",
    "```py\n",
    "class PythonEventsSpider(scrapy.Spider):\n",
    "    name = 'pythoneventsspider'\n",
    "\n",
    "    start_urls = ['https://www.python.org/events/python-events/',]\n",
    "```\n",
    "Every spider is given a name, and also one or more start_urls which tell it where to start the crawling.\n",
    "\n",
    "This spider has a field to store all the events that we find:\n",
    "```py\n",
    "    found_events = []\n",
    "```\n",
    "\n",
    "```python\n",
    "The spider then has a method names parse which will be called for every page the spider collects.\n",
    "\n",
    "def parse(self, response):\n",
    "        for event in response.xpath('//ul[contains(@class, \"list-recent-events\")]/li'):\n",
    "            event_details = dict()\n",
    "            event_details['name'] = event.xpath('h3[@class=\"event-title\"]/a/text()').extract_first()\n",
    "            event_details['location'] = event.xpath('p/span[@class=\"event-location\"]/text()').extract_first()\n",
    "            event_details['time'] = event.xpath('p/time/text()').extract_first()\n",
    "            self.found_events.append(event_details)\n",
    "```\n",
    "\n",
    "The implementation of this method uses and XPath selection to get the events from the page (XPath is the built in means of navigating HTML in Scrapy). It them builds the event_details dictionary object similarly to the other examples, and then adds it to the found_events list.\n",
    "\n",
    "The remaining code does the programmatic execution of the Scrapy crawler.\n",
    "```py\n",
    "    process = CrawlerProcess({ 'LOG_LEVEL': 'ERROR'})\n",
    "    process.crawl(PythonEventsSpider)\n",
    "    spider = next(iter(process.crawlers)).spider\n",
    "    process.start()\n",
    "```\n",
    "\n",
    "It starts with the creation of a CrawlerProcess which does the actual  crawling and a lot of other tasks.  We pass it a `LOG_LEVEL` of `ERROR` to prevent the voluminous Scrapy output.  Change this to `DEBUG` and re-run it to see the difference.\n",
    "\n",
    "Next we tell the crawler process to use our Spider implementation.  We get the actual spider object from that crawler so that we can get the items when the crawl is complete.  And then we kick of the whole thing by calling `process.start()`.\n",
    "\n",
    "When the crawl is completed we can then iterate and print out the items that were found.\n",
    "```py\n",
    "    for event in spider.found_events: print(event)\n",
    "```\n",
    "\n",
    "> This example really didn't touch any of the power of Scrapy.  We will look more into some of the more advanced features later in the book."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping python.org with Selenium**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This recipe will ntroduce Selenium, a framework that is very different from the frameworks in the previour recipes. In fact, Selenium is often used in functional/acceptance testing. We want to demonstrate this tool as it offers unique benefits from the scraping perspective. Several that we will look at later in the book are the ability to fill out forms, press buttons, and wait for dynamic JS to be downloaded and executed.\n",
    "S\n",
    "Selenium itself is a programming language neutral framework. It offers a number of programming language bindings, such as Python, Java, C# and PHP (among others). The framework also provides many components that focus on testing. Three commonly used components are:\n",
    "\n",
    "* IDE for recording and replaying tests\n",
    "* Webdriver, which actually launches a web browser(such as Firefox, Chrome, or Internet Explorer) by sending commands and sending the results to the selected browser\n",
    "* A grid server executes tests with a web browser on a remove server. It can run multiple test cases in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: c:\\Users\\HP\\miniconda3\\envs\\scraping\n",
      "\n",
      "  added / updated specs:\n",
      "    - selenium\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    async_generator-1.10       |             py_0          18 KB  conda-forge\n",
      "    exceptiongroup-1.1.0       |     pyhd8ed1ab_0          18 KB  conda-forge\n",
      "    outcome-1.2.0              |     pyhd8ed1ab_0          12 KB  conda-forge\n",
      "    python_abi-3.7             |          2_cp37m           4 KB  conda-forge\n",
      "    selenium-4.7.2             |     pyhd8ed1ab_0         272 KB  conda-forge\n",
      "    sortedcontainers-2.4.0     |     pyhd8ed1ab_0          26 KB  conda-forge\n",
      "    trio-0.22.0                |   py37h03978a9_0         529 KB  conda-forge\n",
      "    trio-websocket-0.9.2       |     pyhd8ed1ab_0          25 KB  conda-forge\n",
      "    wsproto-1.2.0              |     pyhd8ed1ab_0          24 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         929 KB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  async_generator    conda-forge/noarch::async_generator-1.10-py_0\n",
      "  exceptiongroup     conda-forge/noarch::exceptiongroup-1.1.0-pyhd8ed1ab_0\n",
      "  h11                conda-forge/noarch::h11-0.14.0-pyhd8ed1ab_0\n",
      "  outcome            conda-forge/noarch::outcome-1.2.0-pyhd8ed1ab_0\n",
      "  python_abi         conda-forge/win-64::python_abi-3.7-2_cp37m\n",
      "  sniffio            conda-forge/noarch::sniffio-1.3.0-pyhd8ed1ab_0\n",
      "  sortedcontainers   conda-forge/noarch::sortedcontainers-2.4.0-pyhd8ed1ab_0\n",
      "  trio               conda-forge/win-64::trio-0.22.0-py37h03978a9_0\n",
      "  trio-websocket     conda-forge/noarch::trio-websocket-0.9.2-pyhd8ed1ab_0\n",
      "  wsproto            conda-forge/noarch::wsproto-1.2.0-pyhd8ed1ab_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2022.10.11~ --> conda-forge::ca-certificates-2022.12.7-h5b45459_0\n",
      "  selenium           pkgs/main/win-64::selenium-3.141.0-py~ --> conda-forge/noarch::selenium-4.7.2-pyhd8ed1ab_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/win-64::certifi-2022.12.7-p~ --> conda-forge/noarch::certifi-2022.12.7-pyhd8ed1ab_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "python_abi-3.7       | 4 KB      |            |   0% \n",
      "python_abi-3.7       | 4 KB      | ########## | 100% \n",
      "python_abi-3.7       | 4 KB      | ########## | 100% \n",
      "\n",
      "outcome-1.2.0        | 12 KB     |            |   0% \n",
      "outcome-1.2.0        | 12 KB     | ########## | 100% \n",
      "\n",
      "trio-0.22.0          | 529 KB    |            |   0% \n",
      "trio-0.22.0          | 529 KB    | 3          |   3% \n",
      "trio-0.22.0          | 529 KB    | ##7        |  27% \n",
      "trio-0.22.0          | 529 KB    | ####5      |  45% \n",
      "trio-0.22.0          | 529 KB    | ######9    |  70% \n",
      "trio-0.22.0          | 529 KB    | ########## | 100% \n",
      "trio-0.22.0          | 529 KB    | ########## | 100% \n",
      "\n",
      "trio-websocket-0.9.2 | 25 KB     |            |   0% \n",
      "trio-websocket-0.9.2 | 25 KB     | ########## | 100% \n",
      "trio-websocket-0.9.2 | 25 KB     | ########## | 100% \n",
      "\n",
      "sortedcontainers-2.4 | 26 KB     |            |   0% \n",
      "sortedcontainers-2.4 | 26 KB     | ######2    |  62% \n",
      "sortedcontainers-2.4 | 26 KB     | ########## | 100% \n",
      "\n",
      "exceptiongroup-1.1.0 | 18 KB     |            |   0% \n",
      "exceptiongroup-1.1.0 | 18 KB     | ########## | 100% \n",
      "exceptiongroup-1.1.0 | 18 KB     | ########## | 100% \n",
      "\n",
      "selenium-4.7.2       | 272 KB    |            |   0% \n",
      "selenium-4.7.2       | 272 KB    | #########4 |  94% \n",
      "selenium-4.7.2       | 272 KB    | ########## | 100% \n",
      "\n",
      "async_generator-1.10 | 18 KB     |            |   0% \n",
      "async_generator-1.10 | 18 KB     | ########## | 100% \n",
      "\n",
      "wsproto-1.2.0        | 24 KB     |            |   0% \n",
      "wsproto-1.2.0        | 24 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 22.11.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge selenium"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this lirary to handle the browser driver manager\n",
    "```bash\n",
    "%conda install -c conda-forge webdriver-manager\n",
    "```\n",
    "\n",
    "But here the used the `selenium-manager` developed by the selenium team but to this date still under development.\n",
    "\n",
    "We start by run this command:\n",
    "\n",
    "```bash\n",
    "./selenium-manager --browser edge\n",
    "```\n",
    "This will give us the location of the defined browser driver\n",
    "```\n",
    "INFO    C:\\Users\\HP\\.cache\\selenium\\msedgedriver\\win64\\108.0.1462.54\\msedgedriver.exe\n",
    "```\n",
    "\n",
    "Finally we just add the location of the driver in the code\n",
    "```python\n",
    "driver = webdriver.Edge(service=EdgeService(\"C:/Users/HP/.cache/selenium/msedgedriver/win64/108.0.1462.54/msedgedriver.exe\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Bordeaux, France', 'location': '16 Feb. – 19 Feb.'}\n",
      "{'name': 'Windhoek, Namibia', 'location': '21 Feb. – 23 Feb.'}\n",
      "{'name': 'Manila, Philippines', 'location': '25 Feb. – 26 Feb.'}\n",
      "{'name': 'Basel, Switzerland', 'location': '06 March – 08 March'}\n",
      "{'name': 'Berlin, Germany', 'location': '17 April – 19 April'}\n",
      "{'name': 'Salt Lake City, Utah, USA', 'location': '19 April – 27 April'}\n",
      "{'name': 'Online', 'location': '27 Dec.'}\n",
      "{'name': 'Cochabamba, Bolivia', 'location': '09 Dec. – 10 Dec.'}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.service import Service as EdgeService\n",
    "# from microsoft import EdgeChromiumDriverManager\n",
    "\n",
    "def get_upcoming_events(url):\n",
    "    driver = webdriver.Edge(service=EdgeService(\"C:/Users/HP/.cache/selenium/msedgedriver/win64/108.0.1462.54/msedgedriver.exe\"))\n",
    "    driver.get(url)\n",
    "\n",
    "    events = driver.find_elements('xpath', '//ul[contains(@class, \"list-recent-events\")]/li')\n",
    "\n",
    "    for event in events:\n",
    "        event_details = dict()\n",
    "        event_details['name'] = event.find_element('xpath', 'p/span[@class=\"event-location\"]').text\n",
    "        event_details['location'] = event.find_element('xpath', 'p/time').text\n",
    "        print(event_details)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "get_upcoming_events('https://www.python.org/events/python-events/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f810eed96e0c5f696f0732540cc3a0a0f047188d9a4b9047a527d60a31ac62d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
